\section{Introduction} \label{sec:intro}

% broad context:
% - advance in ai sparked interests in the robotics application, such as
%   self-driving cars.
% - in particular, deep neural network models are increasingly used
%   for perception and control of a vehicle. say. AI workloads.
%
%
Autonomous cars have been a topic of increasing interest in recent
years as many companies are actively developing related hardware
and software technologies toward fully autonomous driving capability with
no human intervention. Deep neural networks (DNNs) have been
successfully applied in various perception and control tasks in
recent years.  They are important workloads for autonomous vehicles
as well. For example, Tesla Model S was known to use a specialized
chip (MobileEye EyeQ), which used a deep neural network for vision-based
real-time obstacle detection and avoidance. More recently, researchers
are investigating DNN based end-to-end control of
cars~\cite{Bojarski2016} and other robots. It is expected that more
DNN based Artificial Intelligence workloads may be used in future
autonomous vehicles.

% big problem
Executing these AI workloads on an embedded computing platform 
poses several additional challenges. First, many AI workloads in vehicles 
are computationally demanding and have strict real-time requirements. 
For example, latency in a vision-based object
detection task may be directly linked to safety of the vehicle. This
requires a high computing capacity as well as the means to guaranteeing
the timings. On the other hand, the computing hardware platform must
also satisfy cost, size, weight, and power constraints, which require a
highly efficient computing platform. These two conflicting
requirements  complicate the platform selection process as observed in
~\cite{Otterness2017}.

%% For example, while today's self-driving car
%% prototype equip more \$100,000
%% of computers and sensors~\cite{juliussen2014emerging}, a study
%% found that aveage consumers are willing to pay much less amount of
%% extra cost for a self-driving capability~\cite{Daziano2017}.
%% https://qz.com/924212/what-it-really-costs-to-turn-a-car-into-a-self-driving-vehicle/

% related work and remaining problems
To understand what kind of computing hardware is needed for AI
workloads, we need a testbed and realistic workloads. While using a real 
car-based testbed would be most ideal, it is not only highly expensive, but also
poses serious safety concerns that hinder development and exploration.
Therefore, there is a strong need for safer and less costly
testbeds. There are already several relatively inexpensive RC-car
based testbeds, such as MIT's 
RaceCar~\cite{shin2017project} and UPenn's F1$/$10 racecar~\cite{upennf1tenth}.
However, these RC-car testbeds still cost more than \$3,000, requiring
considerable investment.

% our goals
Instead, we want to build a low cost testbed that still employs the
state-of-the art AI technologies. Specifically, we focus on a end-to-end
deep learning based real-time control system,
which was developed for a real self-driving car, NVIDIA
DAVE-2~\cite{Bojarski2016}, and use the same methodology on a
smaller and less costly setup. In developing the testbed, our
goals are (1) to analyze real-time issues in DNN based end-to-end
control; and (2) to evaluate real-time performance of contemporary embedded
platforms for such workload.

% DeepPicar introduction
In this paper, we present DeepPicar, a low-cost autonomous car
platform for research. From a hardware perspective,
DeepPicar is comprised of a Raspberry Pi 3 Model B quad-core
computer, a web camera and a RC car, all of which are affordable
components (less than \$100 in total).
The DeepPicar, however, employs state-of-the-art AI
technologies, including a vision-based end-to-end control system that
utilizes a deep convolutional neural network (CNN).
The network receives an image frame from a single forward
looking camera as input and generates a predicted steering angle
value as output at each control period in \emph{real-time}.
The network has 8 layers, about 27 million connections
and 250 thousand parameters (weights).
The network architecture is identical to that of NVIDIA's DAVE-2
self-driving car~\cite{Bojarski2016}, which uses a much more powerful
computer (Drive PX computer~\cite{drivepx}) than a Raspberry Pi~3.
We chose to use a Pi 3 not only because it is affordable, but also because it is representative
of today's mainstream low-end embedded multicore platforms found in
smartphones and other embedded devices.

%% Other than the difference in scale (RC car vs. real car), the only other
%% differences between the two systems---from the computing
%% perspective---are that our system is implemented in
%% TensorFlow~\cite{abadi2016tensorflow} and runs on a Raspberry Pi 3
%% whereas NVIDIA's DAVE-2 systems is implemented in Torch
%% 7~\cite{collobert2011torch7} and runs on a Drive PX computer (NVIDIA's
%% automotive specialized computing system~\cite{drivepx}), which is more
%% powerful but also more expensive.

% how we trained (maybe moved to a later section)
We apply a standard imitation learning methodology to train the car to
follow tracks on the ground. We collect data for
training and validation by manually
controlling the RC car and recording the vision (from the webcam
mounted on the RC-car) and the human control inputs. We then train the
network offline using the collected data on a desktop computer, which
is equipped with a NVIDIA GTX 1060 GPU. Finally, the trained network is copied
back to the Raspberry Pi, which is then used to perform inference
operations---locally on the Pi---in the car's main control loop in
real-time. For real-time control, each inference operation must
be completed within the desired control period (e.g., 33.$\overline{\mbox{33}}$ ms
 period for 30 Hz control frequency).
% how we evaluated (in terms real-time performance)

% what are our findings?
Using the DeepPicar platform, we systematically analyze its real-time
capabilities in the context of deep-learning based real-time
control, especially on real-time deep neural network inferencing.
We also evaluated other, more powerful, embedded computing
platforms to better understand achievable real-time performance of
DeepPicar's deep-learning based control system and the impact of
computing hardware architectures.


We find that the DeepPicar is capable of 
completing control loops in under 33.$\overline{\mbox{33}}$ 
ms, or 30 hz, and can do so 100\% of the time.
Other tested embedded platforms, Intel UP and NVIDIA TX2, offer even
better performance, and are capable of supporting deep-learning based 
real-time control up to 100 Hz control frequency on the TX2 when the GPU 
is used. However, in all platforms, shared resource contention remains 
an important issue as we observe up to 11.6X control loop execution time
increase, mostly due to increase in the neural network inferencing
operation, when memory performance intensive applications are
co-scheduled on idle CPU cores.

% contributions
The {\bf contributions} of this paper are as follows:
\begin{itemize}
  \item We present the design and implementation of a
    low-cost autonomous vehicle testbed, DeepPicar, which utilizes
    state-of-the-art artificial intelligence techniques.
  \item We provide an analysis and case-study of real-time issues in the
    DeepPicar.
  \item We systematically compare real-time computing capabilities of
    multiple embedded computing platforms in the context of
    vision-based autonomous driving.
\end{itemize}

The remaining sections of the paper are as follows: Section II 
provides a background of applications in autonomous driving and related works.
 Section III gives an overview of the DeepPicar platform, including the 
high-level system and the methods used for training and inference. 
Section IV presents our evaluation of the platform and how different 
factors can affect performance. Section V gives a comparison between 
the Raspberry Pi 3 and other embedded computing platforms to 
determine their suitably for autonomous driving research. The paper finishes with 
conclusions in Section VI.
