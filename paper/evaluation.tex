
\section{Evaluation}\label{sec:evaluation}

In this section, we experimentally analyze various real-time aspects
of the DeepPicar. This includes
(1) measurement based worst-case execution time (WCET) analysis of
deep learning inferencing,
(2) the effect of using multiple cores in accelerating inferencing,
(3) the effect of co-scheduling multiple deep neural network models,
and 
(4) the effect of co-scheduling memory bandwidth intensive co-runners.

\subsection{Setup}

The Raspberry Pi 3 Model B platform used in DeepPicar equip a Boardcom
BCM2837 SoC, which has a quad-core 64bit ARM Cortex-A53 cluster,
running at up to 1.2GHz. The chip also includes Broadcom's Videocore IV
GPU, although we did not use the GPU in our evaluation due to the lack
of sofware support (Tensorflow is not compatible with the GPU).
For software, we use Ubuntu MATE 16.04, Tensorflow 1.1 and python
2.7. We disabled DVFS (dynamic voltage frequency scaling) and
configure the clock speed of each core at the maximum 1.2GHz.

\subsection{Inference Timing for Real-Time Control}

For real-time control of a car (or any robots), the control loop
frequency must be sufficiently high so that the car can quickly
react to the changing environment and its internal states. In general,
control performance improves when the frequency is higher, though
computation time and the type of particular physical system are
factors in determining a proper control loop latency. While a standard
control system may be comprized of multiple control loops with
differning control frequencies---e.g., a inner control loop for lower-level
PD control, a outer loop for motion planning, etc.---DeepPicar's
control loop is a single layer, as shown earlier in
Figure~\ref{fig:controlloop}, as a single deep neural network
replaces the traditional multi-layer control pipline. (Recall
Figure~\ref{fig:end-to-end-control} on the differences between the
standard robotics control vs. end-to-end deep learning approach).
This means that the DNN interference operation must be completed
within the inner-most control loop update frequency. To understand
achievable control-loop update frequencies, we experimentally measured
the execution times of DeepPicar's DNN inference operations.

% 50-200Hz for quadcopters:
% https://robotics.stackexchange.com/questions/231/what-frequency-does-my-quadcopter-output-sense-calculate-output-update-loop-need 
% https://quadmeup.com/pid-looptime-why-it-is-not-only-about-frequency/

\begin{figure}[t]
  \centering
  \includegraphics[width=.45\textwidth]{Total_Processing_Time}
  \caption{DeepPicar's control loop processing times over 1000 input image frames.}
  \label{fig:control-loop-timing}
\end{figure}

\begin{table}[t]
  \centering
  \begin{tabular} {| c | r | r | r | r |}
    \hline
    \textbf{Operation} & \textbf{Mean} & \textbf{Max} &   \textbf{99pct.} & \textbf{Stdev.} \\ \hline
    Image capture        & 2.28  &  4.94 &  4.54  & 0.52 \\ \hline
    Image pre-processing & 3.09  &  4.60 &  3.31  & 0.10 \\ \hline
    DNN inferencing      & 37.30 & 51.03 & 45.48  & 2.75 \\ \hline
    Total Time           & 42.67 & 56.37 & 50.70  & 2.80 \\ \hline
  \end{tabular}
  \caption{Control loop timing breakdown.}
  \label{tbl:control-loop-breakdown}
\end{table}

Figure~\ref{fig:control-loop-timing} shows the measured control loop 
processing times of the DeepPicar over 1000 image frames (one per each
control loop). We omit the first frame processing time for cache
warmup. Table~\ref{tbl:control-loop-breakdown} shows the time
breakdown of each control loop. Note that all four CPU cores of the
Raspberry Pi 3 were used by the Tensorflow library when performing the
DNN inference operations.

First, as expected, we find that the inference operation
dominates the control loop execution time, accounting about 85\% of
the execution time.
Second, more importantly, we also find that the measured average
execution time of a single control loop is 42.67 ms, or 23.4 Hz and
the 99 percentail time is 50.70ms.
This means that the DeepPicar can operates
at about 20 Hz control frequency in real-time using only the on-board
Raspberry Pi 3 computing platform, without needing any remote computing
resources. We consider these results respectable given the complexity
of the deep neural network  and the fact that the inference operation
performed by Tensorflow only utilizes the CPU cores of the
Raspberry Pi 3 (its GPU is not supported by Tensorflow.)

In comparison, NVIDIA's DAVE-2 system, which has the exact same neural
network architecture, is reportedly run at 30 Hz~\cite{Bojarski2016},
just a bit faster than the DeepPicar. Although we believe it was not
limited by their computing platform (we will experimentally compare
performance differences among multiple embedded computing platforms,
including NVIDIA's Jetson TK2, later in
Section~\ref{sec:eval-platforms}), the fact that the low-cost
Raspberry Pi 3 can achieve similar real-time control performance is
surprising.

\subsection{Effect of the Core Count to Inference Timing}

In this experiment, we investigate the scalability of performing
inference operation of DeepPicar's nueral network with respect to the
number of cores. As noted earlier, the Raspberry Pi 3 platform has
four Cortex-A53 cores and the Tensorflow 
provides a programmable mechanism to to adjust how many cores to be
used by the library. Leveraging this feature, we repeat the
same experiment described in the previous subsection with varying
number of CPU cores---from one to four.


\begin{figure}[h]
  \centering
  \includegraphics[width=.45\textwidth]{figs/perf_vs_corecnt}
  \caption{Average control loop execution time vs. \#of CPU
    cores. \fixme{instead of the total control loop time, use the
      average of DNN inferencing time.}}
  \label{fig:perf-vs-corecnt}
\end{figure}

%% \begin{table}[h]
%%   \centering
%%   \begin{tabular} {| c | l | l | l | l | l | l | l | l | l |}
%%     \hline
%%     \textbf{\#of cores} & \textbf{Mean} & \textbf{Max} & \textbf{99pct} & \textbf{Stdev} \\ \hline 
%%     1 & 61.96 & 66.00 & 63.31 & 0.51\\ \hline
%%     2 & 50.49 & 71.55 & 70.03 & 3.16 \\ \hline
%%     3 & 48.11 & 72.22 & 58.45 & 4.18 \\ \hline
%%     4 & 42.67 & 56.37 & 50.70 & 2.80 \\
%%     \hline
%%   \end{tabular}
%%   \caption{Execution time statistics vs. \#of CPU cores.}
%% \end{table}

Figure~\ref{fig:perf-vs-corecnt} shows the average execution times of
the control loop as we vary the number of cores used by the
Tensorflow. As expected as we assign more cores, the average execution
time decreases---from 61.96 ms on a single core to 42.67 ms on four
cores. However, the improvement is far from being ideal as we expected
linear scaling. In particular, from 2 cores to 3 cores, the
improvement is mere 2.38ms (or 4\%). In short, we find that the
scalability of DeepPicar's deep neural network is not ideal on the
platform. We do not know whether it is due to the limitations of
Tensorflow's multicore implementation or it's the model's inherent
characteristics. 

The poor scalability opens up the possibility of consolidating
multiple different tasks or different nueral network models rather
than allocating all cores for a single nueral network model. For
example, it is conceivable to use four cameras and four different
neural network models, each of which is trained seperately and
executed on a single dedicated core. Assuming we use the same network
architecture for all models, then one might exepct to achieve up to
15 Hz using one core (given 1 core can deliver 62ms average
execution time). In the next experiment, we investigate the
feasibility of such a scenario.

%% It may not always be the case that all four cores of the Raspberry Pi
%% 3's Cortex A-53 CPU can be used solely for the purpose of operating an
%% autonomous vehicle. Thus, we test how the number of cores utilized for
%% real-time operations affects the DeepPicar's overall ability to
%% function as an autonomous vehicle platform.

%% The performance of the DeepPicar is summarized in Table III. The
%% DeepPicar performed better, on average, when it utilized more
%% cores. With 4 cores, it was able to meet the vast majority of its
%% deadlines, doing so in almost 99\% of the input frames. The platform
%% performed the worst when using only 1 core, as it was unable to meet
%% any of its deadlines.  On average, using 3 cores instead of 2 only had
%% a performance increase of approximately 2 ms, so the addition of one
%% core in that specific case offers  relatively little improvement. One
%% important observation is that the performance was more consistent when
%% only 1 core was used. As a result, the use of multiple cores is very
%% beneficial in terms of reducing the time it takes to complete
%% inference operations, but may result in times that are more volatile.

\subsection{Effect of Co-scheduling Multiple DNN Models}

We also test the capability of the DeepPicar to run multiple models at
the same time, and whether each model can successfully perform within
the given deadline. Specifically, the platform is tested in the cases
of running 2 and 4 models simultaneously. For each case, all models
are allocated an equal number of cores. That is, 2 models are given 2
cores each and 4 models are given 1 core each.

\begin{figure}[h]
  \centering
  \includegraphics[width=.45\textwidth]{figs/perf_vs_modelcnt}
  \caption{Timing impact of co-scheduling multiple DNNs. X-axis shows
    the system configuration: \#of DNN models x \#of CPU cores\/DNN. For
    example, '4x1' means running four DNN models each of which is
    assigned to run on one CPU core, where as '2x2' means running two DNN
    models, each of which is assigned to run on two CPU cores.}
  \label{fig:perf-vs-multimodel}
\end{figure}

%% \begin{figure}[h]
%%   \centering
%%   \includegraphics[width=.5\textwidth]{figs/2ModelChart}
%%   \caption{ Change in inferencing time when 2 models are run concurrently. }
%% \end{figure}

\begin{table}[h]
\centering
  \begin{tabular} {| l | l | l |}
  \hline
  \textbf{num models} & 1 & 2 \\ \hline
  \textbf{L1 refs} &  3.04E+10 &  3.04E+10 \\ \hline
  \textbf{L1 misses} & 4.78E+08 & 4.91E+08 \\ \hline
  \textbf{L1 miss \%} & 1.58 & 1.61 \\ \hline
  \textbf{L2 refs} & 3.31E+09 &  3.91E+09 \\ \hline
  \textbf{L2 misses} & 3.68E+08 & 4.62E+08 \\ \hline
  \textbf{L2 miss \%} & 11.12 & 10.88 \\ \hline
  \end{tabular}
  \caption{ Effect of 2 simulatneous models on cache accesses. }
\end{table}

The results for the 2 model test are outlined in Figure 3 and Table IV, and the 4 model test is summarized in 
Figure 4 Table V. In the the 2 model test, both of the models showed average inference time increases of around 
5-7 ms, $\sim$10\%, when compared to a baseline of 1 model running on two cores. The difference was even 
greater in the 4 model test, as each model displayed an average inference time increase of 
approximately 15 ms, $\sim$30\%, when compared to a single model running on 1 core.

The increase in inference times, however, was not the result of increased cache misses due to 
additional accesses. For all models, the number of L1 misses was always $\sim$1.6\% of all 
references. In the 2 model test, L2 cache misses remained at $\sim$11\% of all references, and each 
model in the 4 model test missed $\sim$13\% of all L2 references.

\begin{figure}[h]
  \centering
  \includegraphics[width=.5\textwidth]{figs/4ModelChart}
  \caption{Change in inferencing time when 4 models are run simultaneously. }
\end{figure}


\begin{table}[h]
\centering
  \begin{tabular} {| l | l | l |}
  \hline
  \textbf{num models} & 1 & 4 \\  \hline
  \textbf{L1 refs} & 2.78E+10 & 2.79E+10 \\ \hline
  \textbf{L1 misses} & 4.36E+08 & 4.53E+08 \\ \hline
  \textbf{L1 miss \%} & 1.57 & 1.63 \\ \hline
  \textbf{L2 refs} & 2.83E+09 & 3.36E+09 \\ \hline 
  \textbf{L2 misses} & 3.59E+08 & 4.43E+08 \\ \hline
  \textbf{L2 miss \%} & 12.68 & 13.19 \\ 
\hline
  \end{tabular}
  \caption{ Effect of 4 concurrent models on cache accesses. }
\end{table}

In all multimodel tests, it was found that a greater number of models run simultaneously resulted in 
interference that led to a noticable increase in the average inferencing time that wasn't 
attributable to a change in cache misses. Instead, the overall time increases can most likely be 
traced to an increase in cache latency. Even if the models had a consistent number of cache hits, it 
is highly probable that the contention for the cache increased the access time for each model, 
consequently increasing the time it took for the models to execute their operations.

In terms of real-time performance under a WCET of 50 ms, the DeepPicar was unsatisfactory in all 
multimodel tests. On average, the 2 model test saw models miss their deadline by 6-8 ms, and the 4 
model test was worse as each model went over their deadlines by 27-29 ms. As a result, it can be 
ascertained that, if a WCET of 50 ms is required, DeepPicar would not be able to perform as necessary.

\subsection{Benchmark Performance}
In order to determine how cache misses affects the performance of the DeepPicar, we test its ability by 
running synthetic benchmarks concurrently with the model. In each experiment, we run a single model on a 
number of cores, and a synthetic benchmark on the remaining idle cores. The effects of the benchmarks can
seen in Figure 9 and Table VI.

\begin{figure}[h]
  \centering
  \includegraphics[width=.5\textwidth]{figs/BenchmarkChart}
  \caption{ Effect of benchmarks on inferencing time. }
\end{figure}

\begin{table}[h]
\centering
  \begin{tabular} {| l | l | l | l | l | l |}
  \hline
  \textbf{num cores} & 1 & 2 & 3 \\ \hline
  \textbf{L1 refs} & 3.16E+10 & 3.17E+10 & 3.06E+10 \\ \hline
  \textbf{L1 misses} & 5.41E+08 & 5.35E+08 & 4.98E+08 \\ \hline
  \textbf{L1 miss \%} & 1.71 & 1.69 & 1.62 \\ \hline
  \textbf{L2 refs} & 3.49E+09 & 3.23E+09 & 3.21E+09 \\ \hline
  \textbf{L2 misses} & 8.05E+08 & 7.54E+08 & 4.98E+08 \\ \hline
  \textbf{L2 miss \%} & 23.05 & 23.37 & 15.51 \\ 
  \hline
  \end{tabular}
  \caption{ Effect of benchmarks on cache accesses. }
\end{table}

The presence of the benchmark(s) had a noticable effect on the DeepPicar, with all tests showing 
increases in inferencing operations. The least change happened when the model was run on 3 cores, and a 
single benchmark was run on the final core. Even with a slight increase in the number of L2 cache 
misses, inferencing was able to be completed in under 100 ms. The other tests, however, showed dramatic 
changes when the model was run on 1 and 2 cores. In both instances, L2 misses rose to $\sim$23\% of all 
references. The additional benchmark in the 1 core test was even more detrimental as the average 
inferencing time was at least 150 ms longer than when the model used 2 cores. 

With the introduction of synthetic benchmarks, the DeepPicar failed to comply with the 50 ms WCET across 
all tests run. Even in the best case, inferencing operations take twice as long as the deadline to 
complete. As a result, if benchmarks were required to run during autonomous operation, the DeepPicar 
wouldn't be capable of meeting its deadlines.

\subsection{Summary}
We found that the DeepPicar is capable of successfully completing all necessary inferencing 
operations within a given deadline of 50 ms. Between these operations, it was discovered that the 
angle prediction time of the model was the dominating step in the processing time of a single frame. 
The other operations were found to execute in very little time, with either one taking 5 ms at most.

When running a single model, DeepPicar performed the best when it used all 4 cpu cores, as it was 
able to process a frame in 43 ms on average. This feat can still be accomplished when running a 
single model on 3 cpu cores, and is also possible when using 2 cores. The only time in which the Pi 
can't run a single model under 50 ms is when only 1 core is used. This was also the case when 
multiple models were run concurrently as no models were able to consistently complete inference 
operations in under 50 ms.

However, the DeepPicar was shown to be capable of handling other potential WCETs. If given a larger 
deadline, the capabilities of DeepPicar as an autonomous vehicle platform would be greatly increased. 
For example, if tested with a deadline of 66.67 ms, or 15 fps, the platform would have passed several 
of the experiments performed. In actuality, the DeepPicar would have only been unsatisfactory in the 
4 model test where it would have missed the deadlines by $\sim$10 ms, and the benchmark tests, where 
it would still miss by $\sim$33 ms at best.

\subsection{Performance Requirements}
In the utilization of the Raspberry Pi 3 in our platform, there are a few factors that need to be 
considered and/or enforced in order to guarantee that the Pi is able to consistently perform at a 
desired level. Specifically, these issues all have the potential to negatively affect the cpu clock 
speed/frequency, which would result in decreased performance. While, in the above experiments, the cpu 
operated at a preferred clock speed of 1.2 GHz, it is entirely possible for the cpu to operate at a 
lower frequency if the following problems are not taken into account.

The most notable issue that can affect the cpu clock speed is that of the power supplied to the 
Raspberry Pi. In essence, it is necessary that the Pi be supplied with 2 Amps, as any less could 
hinder the Pi's ability to maintain a 1.2 GHz frequency. In experiments conducted with a power supply that 
only provided 1 Amp, the Pi was unable to sustain a 1.2 GHz clock speed and, instead, fluctuated 
between operating at 600 MHz and 1.2 GHz. As a result, it is necessary, or at least highly 
recommended, that the power supply used for the Raspberry Pi 3 be capable of outputting 2 Amps, 
otherwise optimal performance isn't guaranteed.

Another factor that can affect clock speed is that of the cpu's temperature. Some model operations can 
be computationally intensive, thus it is possible for the temperature of the cpu to become relatively 
high. This can be especially problematic in situations where multiple models are running 
simultaneously on the Pi. Consequently, thermal throttling may be used to decrease the clock 
speed so that the cpu temperature stays at a safe level. Thus the Raspberry Pi may not be suited 
for prolonged use, especially in cases where the workload is relatively larger, such as running multiple 
models. Rather, the Pi seems to be better suited for running in set periods, after which it is turned 
off or made idle so that the cpu is allowed time to cool down.

\subsection{System Comparisons}
We compare the real time performance of the Raspberry Pi 3 to two other embedded computing platforms, 
the Intel UP Board the NVIDIA Tegra TX2, to evaluate their efficacy in vision-based autonomous 
vehicles. We conduct the same experiments done on the DeepPicar with each platform, and analyze the 
results.

\begin{figure}[h]
  \centering
  \includegraphics[width=.5\textwidth]{figs/system_multicore}
  \caption{Inferencing time across platforms based on number of cores used.}
  \label{fig:sys_core}
\end{figure}

The results of the multicore tests can be seen in Figure~\ref{fig:sys_core}. It was found that the 
Intel UP Board performed better in all experiments. On average, it was able to perform inferencing 
operations, at least, twice as fast as the Raspberry Pi 3. As a result, the UP Board was able to 
satisfy the 50 ms WCET by a very clear margin (in the worst case, it would still meet it by $\sim$20 
ms), and, thus, demonstrates that it is more capable of autonomous driving than the Raspberry Pi 3.

The Intel UP Board also performed better in the multimodel tests, as is shown in 
Figure~\ref{fig:sys_model}. Once again, the UP was able to complete inferecing operations in half the 
time when compared to the Pi. Furthermore, the Intel UP Board was capable of satisfying the 50 ms 
WCET, while the Raspberry Pi 3 was unable to do so. As such, the Intel UP Board displays greater 
potential for the simultaneous execution of multiple models during self-driving operation, while the 
Raspberry Pi 3 would struggle to do so.

\begin{figure}[h]
  \centering
  \includegraphics[width=.5\textwidth]{figs/system_multimodel}
  \caption{Inferencing time across platforms based on number of models run concurrently.}
  \label{fig:sys_model}
\end{figure}

Finally, we examined if the performance of the Intel UP Board and Tegra TX2 would be affected by the 
addition of the synthetic benchmarks. As summarized in Figure~\ref{fig;sys_bench}, the Intel UP Board 
did experience a change in its inferencing times, but the increase was not as drastic as is seen by 
the Raspberry Pi 3. in the worst case, the Intel UP Board produced times that were over two times as 
large, whereas the Raspberry Pi 3 output times that were around 10 times greater. The Intel UP Board 
was successful in completing inferencing operations in under 50 ms when benchmarks were run on a 
maximum of 2 cores, while the Raspberry Pi 3 failed to do so when any benchmark was introduced. As 
such, it can be concluded that the Intel UP Board would be more capable of running computationally 
heavy processes during autonomous driving, and that the Pi wouldn't be able to do the same.

\begin{figure}[h]
  \centering
  \includegraphics[width=.5\textwidth]{figs/system_benchmark}
  \caption{Inferencing time across platforms based on number of cores used.}
  \label{fig:sys_bench}
\end{figure} 

In the comparison of the real-time capabilities of three embedded computing platforms, we found that 
the Raspberry Pi 3 performed the worst. When compared to the Intel UP Board, inferencing on the Pi 
took twice as long across all multicore and multimodel experiments. The difference was even more 
noticeable in the the addition of computationally heavy synthetic benchmarks that had a much more 
dire effect on the Pi.




%% In our platform, three main real-time tasks are performed during autonomous operation. In order, these 
%% operations are: (1) capturing and reading the input frame from the designated camera or video stream, 
%% (2) preprocessing the acquired frame so that it is compatible with the DNN, and (3) feeding the frame 
%% to, and getting the angle prediction from, the model. We aim to determine which operation(s), if any, 
%% require the most time to be performed.

%% In order to determine which operation(s) take the longest to execute, we measured the time it 
%% took for each step to be completed. For this experiment, all four of the Pi's cpu cores were utilized, 
%% and only one model was run. As is shown in Table II, the angle prediction operation consumes the 
%% majority of the processing for each frame. Furthermore, the time it takes for the operation to 
%% complete is volatile, and can range anywhere between 30 ms and 50 ms for any particular frame. On the 
%% other hand, both the frame capture and preprocessing operations take substantially less time 
%% and are relatively more consistent in their times, at 2 ms and 3 ms, respectively. 


%% In evaluating the real-time efficacy of the DeepPicar, the methodology is consistent across all 
%% experiments. The performance of the platform is measured over a set of 1001 video frames that are 
%% each individually fed to the model. The processing time for the first frame is omitted as, due to 
%% cache warmup, it is uncharacteristically high and doesn't
%% accurately represent the Picar's capabilities.  
%% A WCET of 50 ms, or 20 Hz, is used as a baseline to assess the platform's ability to complete all 
%% necessary real-time operations in a timely manner.

%% We seek to determine if the DeepPicar is capable of consistently executing all necessary functions 
%% before their given deadlines. In the case of our platform, it has to process every given frame and get 
%% the predicted angles from the model within a WCET of 50 ms. In order to determine if this was achievable, 
%% we tested the platform by running a single model that utilized all 4 cpu cores and measured the time it 
%% took for each frame to be processed. The performance of the DeepPicar can be seen in Figure 6. We found 
%% that DeepPicar was able to completely process the vast majority of the provided frames within 50 ms, and 
%% was unable to do so for very few frames.

%% In our platform, three main real-time tasks are performed during autonomous operation. In order, these 
%% operations are: (1) capturing and reading the input frame from the designated camera or video stream, 
%% (2) preprocessing the acquired frame so that it is compatible with the DNN, and (3) feeding the frame 
%% to, and getting the angle prediction from, the model. We aim to determine which operation(s), if any, 
%% require the most time to be performed.

%% In order to determine which operation(s) take the longest to execute, we measured the time it 
%% took for each step to be completed. For this experiment, all four of the Pi's cpu cores were utilized, 
%% and only one model was run. As is shown in Table II, the angle prediction operation consumes the 
%% majority of the processing for each frame. Furthermore, the time it takes for the operation to 
%% complete is volatile, and can range anywhere between 30 ms and 50 ms for any particular frame. On the 
%% other hand, both the frame capture and preprocessing operations take substantially less time 
%% and are relatively more consistent in their times, at 2 ms and 3 ms, respectively. 
